#ML
# SGD - Metoda nejvyÅ¡Å¡Ã­ho spÃ¡du
Metoda nejvÄ›tÅ¡Ã­ho spÃ¡du. NumerickÃ© iterativnÃ­ Å™eÅ¡enÃ­.
HrozÃ­ najitÃ­ lokÃ¡lnÃ­ho minima. TrpÃ­ zig-zag efektem pro pÅ™Ã­liÅ¡ velkÃ½ krok alfa.

$$
ğ‘¥_{ğ‘¡+1} = ğ‘¥_ğ‘¡ âˆ’ \alpha \frac{ğ‘‘}{ğ‘‘ğ‘¥} ğ¿(ğ‘¥_ğ‘¡)
$$
- $\alpha$ - (alfa) velikost kroku

## Zig-zag
Efekt nastÃ¡vÃ¡, pokud je optimÃ¡lnÃ­ smÄ›r konvergence kolmÃ½ na gradient. Konvergence pak probÃ­hÃ¡ smÄ›rem k minimu ale dochÃ¡zÃ­ k oscilaci.

## DalÅ¡Ã­ metody:
### momentum
"SetrvaÄnost"
PotlaÄuje oscilace a zrychluje konvergenci. K aktuÃ¡lnÃ­mu kroku se pÅ™idÃ¡ 90% pÅ™edchozÃ­ho. 

### adagrad

## UÄenÃ­
```Python
# ğœƒ ... theta
# Loss funkce:
L(ğœƒ) = sum((yn - ğœƒ^T*~xn)^2)

# Gradient:
-2*sum((yn - ğœƒ^T*~xn)*~xn)

# Algoritmus:
ğœƒ[t+1] = ğœƒ[t] - alfa * sum((yn - ğœƒ^T*~xn)*~xn)
```

## Zdroje:
- [Gradient descent, how neural networks learn | Chapter 2, Deep learning](https://www.youtube.com/watch?v=IHZwWFHWa-w)