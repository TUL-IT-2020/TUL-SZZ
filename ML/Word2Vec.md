#MVD 
# Word2Vec 
Vyu≈æ√≠v√° neuronovou s√≠≈• s jednou skrytou vrstvou. Dva mo≈æn√© algoritmy: 
- Skip-gram 
	- Zalo≈æen na predikce slov v okol√≠ dan√©ho slova 
- Continuous bag of words (CBOW)
	- Zalo≈æen na predikci dan√©ho slova pomoc√≠ okoln√≠ch slov

Nƒõkolik tr√©novac√≠ch metod:
- [[Softmax|Softmax]]
- Hierarchical Softmax
- Negative sampling

## Skip-gram 
Maximalizujme pravdƒõpodobnost v√Ωskytu okoln√≠ch slov
### P≈ô√≠klad:
*Petr dnes ≈°el do kina*
P≈ôi ≈°√≠≈ôce okol√≠ 2 maximalizujeme pro slovo ≈°el pravdƒõpodobnost:
ùëÉ(ùëÉùëíùë°ùëü‚îÇ≈°ùëíùëô,ùëæ)+ùëÉ(ùëëùëõùëíùë†‚îÇ≈°ùëíùëô,ùëæ)+ùëÉ(ùëëùëú‚îÇ≈°ùëíùëô,ùëæ)+ùëÉ(kina | ≈°el,ùëæ)

Model p≈ôitom odpov√≠d√° neuronov√© s√≠ti s parametry $ùëæ$. Parametry $ùëæ$ jsou matice vah skryt√© a v√Ωstupn√≠ vrstvy ($ùëæ1$ a $ùëæ2$ ).
Matice $ùëæ1$ pak p≈ôedstavuje matici embedding≈Ø pro v≈°echna slova ze slovn√≠ku. Jej√≠ rozmƒõry (poƒçet neuron≈Ø skryt√© vrstvy) urƒçuj√≠ dimenzi nalezen√©ho prostoru. Nalezen√Ω prostor m√° po≈æadovan√© vlastnosti.

## Nev√Ωhody popsan√©ho zp≈Øsobu tr√©nov√°n√≠ 
V√Ω≈°e popsan√© tr√©nov√°n√≠ m√° nev√Ωhody: 
1. V√Ωpoƒçet softmaxu pro velk√© slovn√≠ku je n√°roƒçn√Ω (exponenci√°la) 
2. Pro dan√© c√≠lov√© slovo se v√Ωznamnƒõ mƒõn√≠ hodnoty pouze omezen√©ho poƒçtu vah, ostatn√≠ v√°hy se p≈ôesto aktualizuj√≠ nadbyteƒçnƒõ o zanedbatelnƒõ mal√© hodnoty 

P≈ô√≠klad:
P≈ôed tr√©novan√Ω Google model m√° 3 miliony slov. P≈ôi tr√©nov√°n√≠ je pouze 1 slovo ze 3M c√≠lov√©. Vƒõt≈°inu ƒçasu aktualizujeme v√°hy, kter√© nesouvis√≠ s c√≠lov√Ωm slovem o mal√© hodnoty (viz 2.)
≈òe≈°en√≠ ve v√Ωbƒõru tr√©novac√≠ metody: 
- **Hierarchical softmax** -> Slo≈æitƒõj≈°√≠ na implementaci 
- **Negative sampling** -> Dosahuje lep≈°√≠ch v√Ωsledk≈Ø
