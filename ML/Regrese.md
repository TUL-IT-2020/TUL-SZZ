# Regrese
## LSE - algebraick칠 콏e코en칤
metodou nejmen코칤ch 캜tverc콢 = Least-Squares Estimation
Nev칳hodou metody je nutnost po캜칤tat inverzi pro ohromn칠 matice (v칳po캜etn캩 n치ro캜n칠, zat칤쬰n numerickou chybou).

Polynomi치ln칤 regrese
```Python
y = theta[0] + theta[1]*x
```

### Chyba
- loss funkce
```Python
L(洧랚) = sum((yi - yni)^2) = sum( (yi - 洧랚[0] - 洧랚[1]*x)^2 )
```

### U캜en칤
Hled치me takovou thetu, aby chyba byla minim치ln칤
```Python
洧랚 = (~X^T*~X)^1 * ~X^T*Y
```

## SGD - numerick칠 iterativn칤
Metoda nejv캩t코칤ho sp치du.
Hroz칤 najit칤 lok치ln칤ho minima. Trp칤 zig-zag efektem pro p콏칤li코 velk칳 krok alfa.
alfa - velikost kroku

**Momentum**
"Setrva캜nost"
Potla캜uje oscilace a zrychluje konvergenci.
K aktu치ln칤mu kroku se p콏id치 90% p콏edchoz칤ho. 

### U캜en칤
```Python
# 洧랚 ... theta
# Loss funkce:
L(洧랚) = sum((yn - 洧랚^T*~xn)^2)

# Gradient:
-2*sum((yn - 洧랚^T*~xn)*~xn)

# Algoritmus:
洧랚[t+1] = 洧랚[t] - alfa * sum((yn - 洧랚^T*~xn)*~xn)
```