#ML 
# Regrese
## LSE - algebraick칠 콏e코en칤
[[Bayesovsk치 statistika#Minimum Mean Square Error|MSE?]]
metodou nejmen코칤ch 캜tverc콢 = Least-Squares Estimation 
Nev칳hodou metody je nutnost po캜칤tat inverzi pro ohromn칠 matice (v칳po캜etn캩 n치ro캜n칠, zat칤쬰n numerickou chybou).

Polynomi치ln칤 regrese
```Python
y = theta[0] + theta[1]*x
```

### Chyba
```Python
L(洧랚) = sum((yi - yni)^2) = sum( (yi - 洧랚[0] - 洧랚[1]*x)^2 )
```

$$
L(\theta) = \sum_{i=1}^{N}(y_i - \hat y_i)^2
$$
- $L$ - Loss funkce
- $\hat y_i$ - predikovan치 hodnota

### U캜en칤
Chyba modelu p콏edstavuje kriteri치ln칤 funkci. Minimalizovat tuto funkci vzhledem k parametr콢m modelu znamen치 naj칤t takov칠 hodnoty parametr콢, aby v칳sledn치 chyba byla minim치ln칤. Naj칤t minimum t칠to funkce znamen치 vyj치d콏it jej칤 prvn칤 derivaci vzhledem k hledan칳m parametr콢m a polo쬴t ji rovnu nule. 
$$
\theta_洧릝 = \overline{洧눜} - \theta_洧릞 * \overline{x} = mean(洧) - \theta_洧릞 * mean(X)
$$

V칤cen치sobn치 LR (line치rn칤 regrese): LSE pro $\theta$
$$
\theta = \left(\widetilde{X}^T*\widetilde{X}\right)^{-1} * \left(\widetilde{X}^T*Y\right)
$$

## SGD
![[SGD - Metoda nejvy코코칤ho sp치du]]
